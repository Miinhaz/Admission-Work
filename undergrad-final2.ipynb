{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":58429,"sourceType":"datasetVersion","datasetId":38367},{"sourceId":1874598,"sourceType":"datasetVersion","datasetId":1115942},{"sourceId":6476455,"sourceType":"datasetVersion","datasetId":3741302},{"sourceId":6477512,"sourceType":"datasetVersion","datasetId":3742034},{"sourceId":6935927,"sourceType":"datasetVersion","datasetId":3982915},{"sourceId":7274220,"sourceType":"datasetVersion","datasetId":2814684},{"sourceId":7790262,"sourceType":"datasetVersion","datasetId":4559917}],"dockerImageVersionId":30498,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/undergrad/Undergraduate.csv\")\n\n# Assuming the last column is the target variable\ntarget_column = df.columns[-1]\n\n# Remove leading and trailing spaces from the target column\ndf[target_column] = df[target_column].str.strip()\n\n# Separate features (X) and target variable (y)\nX = df.drop(columns=[target_column])\ny = df[target_column]\n\n# Identify categorical columns\ncategorical_columns = X.select_dtypes(include=['object']).columns\n\n# One-hot encode categorical columns\nX_encoded = pd.get_dummies(X, columns=categorical_columns)\n\n# Label encode the target variable\nle = LabelEncoder()\ny = le.fit_transform(y)  # Assuming y contains class labels 0 and 1\n\n# Exclude instances where the true class is -1\nvalid_indices = (y != -1)\nX_encoded = X_encoded[valid_indices]\ny = y[valid_indices]\n\n# Perform train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n\n# Data preprocessing\nimputer = SimpleImputer(strategy='mean')\nX_train_imputed = imputer.fit_transform(X_train)\nX_test_imputed = imputer.transform(X_test)\n\nscaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train_imputed)\nX_test_scaled = scaler.transform(X_test_imputed)\n\n# ELM implementation\nclass ELMClassifierManual:\n    def __init__(self, input_size, hidden_size):\n        self.input_size = input_size\n        self.hidden_size = hidden_size\n        self.input_weights = np.random.rand(input_size, hidden_size)\n        self.bias = np.random.rand(hidden_size)\n        self.output_weights = None\n\n    def train(self, X, y):\n        hidden_layer_output = self.sigmoid(X.dot(self.input_weights) + self.bias)\n        self.output_weights = np.linalg.pinv(hidden_layer_output).dot(y)\n\n    def predict(self, X):\n        hidden_layer_output = self.sigmoid(X.dot(self.input_weights) + self.bias)\n        predictions = hidden_layer_output.dot(self.output_weights)\n        return np.round(predictions).astype(int)\n\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n\n# Initialize ELMClassifier\nelm_classifier = ELMClassifierManual(input_size=X_train_scaled.shape[1], hidden_size=100)\n\n# Train ELMClassifier\nelm_classifier.train(X_train_scaled, y_train)\n\n# Predictions\nelm_predictions = elm_classifier.predict(X_test_scaled)\n\n# Exclude instances where the true class is -1 during evaluation\nvalid_test_indices = (y_test != -1)\ny_test_valid = y_test[valid_test_indices]\nelm_predictions_valid = elm_predictions[valid_test_indices]\n\n# Evaluate ELMClassifier\nelm_accuracy = accuracy_score(y_test_valid, elm_predictions_valid)\nelm_report = classification_report(y_test_valid, elm_predictions_valid)\n\nprint(\"ELM Classifier (Manual Implementation):\")\nprint(f\"Accuracy: {elm_accuracy:.2f}\")\nprint(f\"Classification Report:\\n{elm_report}\\n{'='*40}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:38:19.609297Z","iopub.execute_input":"2024-03-12T10:38:19.609686Z","iopub.status.idle":"2024-03-12T10:38:19.678725Z","shell.execute_reply.started":"2024-03-12T10:38:19.609656Z","shell.execute_reply":"2024-03-12T10:38:19.677440Z"},"trusted":true},"execution_count":42,"outputs":[{"name":"stdout","text":"ELM Classifier (Manual Implementation):\nAccuracy: 0.83\nClassification Report:\n              precision    recall  f1-score   support\n\n           0       0.77      0.69      0.73        39\n           1       0.86      0.90      0.88        80\n\n    accuracy                           0.83       119\n   macro avg       0.81      0.80      0.80       119\nweighted avg       0.83      0.83      0.83       119\n\n========================================\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import make_pipeline\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/undergrad/Undergraduate.csv\")\n\n# Assuming the last column is the target variable\ntarget_column = df.columns[-1]\n\n# Remove leading and trailing spaces from the target column\ndf[target_column] = df[target_column].str.strip()\n\n# Separate features (X) and target variable (y)\nX = df.drop(columns=[target_column])\ny = df[target_column]\n\n# Identify categorical columns\ncategorical_columns = X.select_dtypes(include=['object']).columns\n\n# One-hot encode categorical columns\nX_encoded = pd.get_dummies(X, columns=categorical_columns)\n\n# Label encode the target variable\nle = LabelEncoder()\ny = le.fit_transform(y)\n\n# Perform train-test split\nX_train, X_test, y_train, y_test = train_test_split(X_encoded, y, test_size=0.2, random_state=42)\n\n\n# Hyperparameter tuning for SVM\nparam_grid_svm = {\n    'svc__C': [0.1, 1, 10],\n    'svc__kernel': ['linear', 'rbf', 'poly'],\n    'svc__gamma': ['scale', 'auto']\n}\n\n# Hyperparameter tuning for MLPClassifier\nparam_grid_mlp = {\n    'mlpclassifier__hidden_layer_sizes': [(50,), (100, 50), (100, 50, 20)],\n    'mlpclassifier__max_iter': [200, 500, 1000],\n    'mlpclassifier__alpha': [0.0001, 0.001, 0.01]\n}\n\n# Hyperparameter tuning for DecisionTreeClassifier\nparam_grid_dt = {\n    'decisiontreeclassifier__max_depth': [None, 5, 10, 20],\n    'decisiontreeclassifier__min_samples_split': [2, 5, 10],\n    'decisiontreeclassifier__min_samples_leaf': [1, 2, 4]\n}\n\n# Hyperparameter tuning for LogisticRegression\nparam_grid_lr = {\n    'logisticregression__C': [0.1, 1, 10],\n    'logisticregression__max_iter': [50, 100, 200]\n}\n\n# Hyperparameter tuning for GaussianNB (No hyperparameters to tune)\n\n# Hyperparameter tuning for KNeighborsClassifier\nparam_grid_knn = {\n    'kneighborsclassifier__n_neighbors': [3, 5, 7],\n    'kneighborsclassifier__weights': ['uniform', 'distance']\n}\n\n# Hyperparameter tuning for AdaBoostClassifier\nparam_grid_adaboost = {\n    'adaboostclassifier__n_estimators': [50, 100, 200],\n    'adaboostclassifier__learning_rate': [0.1, 0.5, 1]\n}\n\n# Hyperparameter tuning for GradientBoostingClassifier\nparam_grid_gb = {\n    'gradientboostingclassifier__n_estimators': [50, 100, 200],\n    'gradientboostingclassifier__learning_rate': [0.01, 0.1, 0.5],\n    'gradientboostingclassifier__max_depth': [3, 5, 10]\n}\n\n# Create pipelines for each classifier\n\npipeline_svm = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    SVC(random_state=42)\n)\n\npipeline_mlp = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    MLPClassifier(random_state=42)\n)\n\npipeline_dt = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    DecisionTreeClassifier(random_state=42)\n)\n\npipeline_lr = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    LogisticRegression(random_state=42)\n)\n\npipeline_nb = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    GaussianNB()\n)\n\npipeline_knn = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    KNeighborsClassifier()\n)\n\npipeline_adaboost = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    AdaBoostClassifier(random_state=42)\n)\n\npipeline_gb = make_pipeline(\n    SimpleImputer(strategy='mean'),\n    StandardScaler(),\n    GradientBoostingClassifier(random_state=42)\n)\n\n# Create dictionaries for classifiers and their respective hyperparameter grids\nclassifiers = {\n    'SVM': (pipeline_svm, param_grid_svm),\n    'MLPClassifier': (pipeline_mlp, param_grid_mlp),\n    'Decision Tree': (pipeline_dt, param_grid_dt),\n    'Logistic Regression': (pipeline_lr, param_grid_lr),\n    'Naive Bayes': (pipeline_nb, {}),\n    'K-Nearest Neighbors': (pipeline_knn, param_grid_knn),\n    'AdaBoost': (pipeline_adaboost, param_grid_adaboost),\n    'Gradient Boosting': (pipeline_gb, param_grid_gb)\n}\n\n# Loop through classifiers\nfor name, (pipeline, param_grid) in classifiers.items():\n    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n    grid_search.fit(X_train, y_train)\n\n    # Get the best parameters\n    best_params = grid_search.best_params_\n    print(f\"Best Parameters for {name}: {best_params}\")\n\n    # Evaluate the classifier with the best parameters\n    best_classifier = grid_search.best_estimator_\n    best_predictions = best_classifier.predict(X_test)\n    best_accuracy = accuracy_score(y_test, best_predictions)\n    # Convert numeric labels back to class names\n    class_names = le.classes_\n    y_test_names = le.inverse_transform(y_test)\n    best_predictions_names = le.inverse_transform(best_predictions)\n\n    best_report = classification_report(y_test_names, best_predictions_names, target_names=class_names)\n\n    print(f\"{name} Classifier:\")\n    print(f\"Best Parameters: {best_params}\")\n    print(f\"Accuracy: {best_accuracy:.2f}\")\n    print(f\"Classification Report:\\n{best_report}\\n{'='*40}\")","metadata":{"execution":{"iopub.status.busy":"2024-03-12T10:41:31.361845Z","iopub.execute_input":"2024-03-12T10:41:31.362570Z","iopub.status.idle":"2024-03-12T10:45:29.676958Z","shell.execute_reply.started":"2024-03-12T10:41:31.362538Z","shell.execute_reply":"2024-03-12T10:45:29.675934Z"},"trusted":true},"execution_count":43,"outputs":[{"name":"stdout","text":"Best Parameters for SVM: {'svc__C': 10, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}\nSVM Classifier:\nBest Parameters: {'svc__C': 10, 'svc__gamma': 'scale', 'svc__kernel': 'linear'}\nAccuracy: 0.84\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.78      0.72      0.75        39\n Public University       0.87      0.90      0.88        80\n\n          accuracy                           0.84       119\n         macro avg       0.82      0.81      0.82       119\n      weighted avg       0.84      0.84      0.84       119\n\n========================================\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Best Parameters for MLPClassifier: {'mlpclassifier__alpha': 0.0001, 'mlpclassifier__hidden_layer_sizes': (50,), 'mlpclassifier__max_iter': 200}\nMLPClassifier Classifier:\nBest Parameters: {'mlpclassifier__alpha': 0.0001, 'mlpclassifier__hidden_layer_sizes': (50,), 'mlpclassifier__max_iter': 200}\nAccuracy: 0.82\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.70      0.77      0.73        39\n Public University       0.88      0.84      0.86        80\n\n          accuracy                           0.82       119\n         macro avg       0.79      0.80      0.80       119\n      weighted avg       0.82      0.82      0.82       119\n\n========================================\nBest Parameters for Decision Tree: {'decisiontreeclassifier__max_depth': 5, 'decisiontreeclassifier__min_samples_leaf': 4, 'decisiontreeclassifier__min_samples_split': 10}\nDecision Tree Classifier:\nBest Parameters: {'decisiontreeclassifier__max_depth': 5, 'decisiontreeclassifier__min_samples_leaf': 4, 'decisiontreeclassifier__min_samples_split': 10}\nAccuracy: 0.82\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.72      0.72      0.72        39\n Public University       0.86      0.86      0.86        80\n\n          accuracy                           0.82       119\n         macro avg       0.79      0.79      0.79       119\n      weighted avg       0.82      0.82      0.82       119\n\n========================================\nBest Parameters for Logistic Regression: {'logisticregression__C': 1, 'logisticregression__max_iter': 50}\nLogistic Regression Classifier:\nBest Parameters: {'logisticregression__C': 1, 'logisticregression__max_iter': 50}\nAccuracy: 0.82\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.75      0.69      0.72        39\n Public University       0.86      0.89      0.87        80\n\n          accuracy                           0.82       119\n         macro avg       0.80      0.79      0.80       119\n      weighted avg       0.82      0.82      0.82       119\n\n========================================\nBest Parameters for Naive Bayes: {}\nNaive Bayes Classifier:\nBest Parameters: {}\nAccuracy: 0.80\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.70      0.67      0.68        39\n Public University       0.84      0.86      0.85        80\n\n          accuracy                           0.80       119\n         macro avg       0.77      0.76      0.77       119\n      weighted avg       0.80      0.80      0.80       119\n\n========================================\nBest Parameters for K-Nearest Neighbors: {'kneighborsclassifier__n_neighbors': 3, 'kneighborsclassifier__weights': 'uniform'}\nK-Nearest Neighbors Classifier:\nBest Parameters: {'kneighborsclassifier__n_neighbors': 3, 'kneighborsclassifier__weights': 'uniform'}\nAccuracy: 0.76\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.65      0.62      0.63        39\n Public University       0.82      0.84      0.83        80\n\n          accuracy                           0.76       119\n         macro avg       0.73      0.73      0.73       119\n      weighted avg       0.76      0.76      0.76       119\n\n========================================\nBest Parameters for AdaBoost: {'adaboostclassifier__learning_rate': 0.1, 'adaboostclassifier__n_estimators': 50}\nAdaBoost Classifier:\nBest Parameters: {'adaboostclassifier__learning_rate': 0.1, 'adaboostclassifier__n_estimators': 50}\nAccuracy: 0.83\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.79      0.67      0.72        39\n Public University       0.85      0.91      0.88        80\n\n          accuracy                           0.83       119\n         macro avg       0.82      0.79      0.80       119\n      weighted avg       0.83      0.83      0.83       119\n\n========================================\nBest Parameters for Gradient Boosting: {'gradientboostingclassifier__learning_rate': 0.1, 'gradientboostingclassifier__max_depth': 3, 'gradientboostingclassifier__n_estimators': 50}\nGradient Boosting Classifier:\nBest Parameters: {'gradientboostingclassifier__learning_rate': 0.1, 'gradientboostingclassifier__max_depth': 3, 'gradientboostingclassifier__n_estimators': 50}\nAccuracy: 0.87\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.81      0.77      0.79        39\n Public University       0.89      0.91      0.90        80\n\n          accuracy                           0.87       119\n         macro avg       0.85      0.84      0.85       119\n      weighted avg       0.86      0.87      0.86       119\n\n========================================\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\n\n# Define the best classifiers based on the results of hyperparameter tuning\nbest_svm = classifiers['SVM'][0]\nbest_svm_keys = [key.split('__')[1] for key in grid_search.best_params_.keys() if key.startswith('svc__')]\nfor key in best_svm_keys:\n    setattr(best_svm.named_steps['svc'], key, grid_search.best_params_['svc__' + key])\n\nbest_mlp = classifiers['MLPClassifier'][0]\nbest_mlp_keys = [key.split('__')[1] for key in grid_search.best_params_.keys() if key.startswith('mlpclassifier__')]\nfor key in best_mlp_keys:\n    setattr(best_mlp.named_steps['mlpclassifier'], key, grid_search.best_params_['mlpclassifier__' + key])\n\nbest_dt = classifiers['Decision Tree'][0]\nbest_dt_keys = [key.split('__')[1] for key in grid_search.best_params_.keys() if key.startswith('decisiontreeclassifier__')]\nfor key in best_dt_keys:\n    setattr(best_dt.named_steps['decisiontreeclassifier'], key, grid_search.best_params_['decisiontreeclassifier__' + key])\n\nbest_lr = classifiers['Logistic Regression'][0]\nbest_lr_keys = [key.split('__')[1] for key in grid_search.best_params_.keys() if key.startswith('logisticregression__')]\nfor key in best_lr_keys:\n    setattr(best_lr.named_steps['logisticregression'], key, grid_search.best_params_['logisticregression__' + key])\n\nbest_knn = classifiers['K-Nearest Neighbors'][0]\nbest_knn_keys = [key.split('__')[1] for key in grid_search.best_params_.keys() if key.startswith('kneighborsclassifier__')]\nfor key in best_knn_keys:\n    setattr(best_knn.named_steps['kneighborsclassifier'], key, grid_search.best_params_['kneighborsclassifier__' + key])\n\nbest_adaboost = classifiers['AdaBoost'][0]\nbest_adaboost_keys = [key.split('__')[1] for key in grid_search.best_params_.keys() if key.startswith('adaboostclassifier__')]\nfor key in best_adaboost_keys:\n    setattr(best_adaboost.named_steps['adaboostclassifier'], key, grid_search.best_params_['adaboostclassifier__' + key])\n\nbest_gb = classifiers['Gradient Boosting'][0]\nbest_gb_keys = [key.split('__')[1] for key in grid_search.best_params_.keys() if key.startswith('gradientboostingclassifier__')]\nfor key in best_gb_keys:\n    setattr(best_gb.named_steps['gradientboostingclassifier'], key, grid_search.best_params_['gradientboostingclassifier__' + key])\n\n# Create an ensemble classifier using a VotingClassifier\nensemble_classifier = VotingClassifier(\n    estimators=[\n        ('SVM', best_svm),\n        ('MLPClassifier', best_mlp),\n        ('Decision Tree', best_dt),\n        ('Logistic Regression', best_lr),\n        ('K-Nearest Neighbors', best_knn),\n        ('AdaBoost', best_adaboost),\n        ('Gradient Boosting', best_gb)\n    ],\n    voting='hard'  # You can change to 'soft' if classifiers provide probability estimates\n)\n\n# Train the ensemble classifier on the training data\nensemble_classifier.fit(X_train, y_train)\n\n\n# Evaluate the ensemble classifier on the test data\nensemble_predictions = ensemble_classifier.predict(X_test)\nensemble_accuracy = accuracy_score(y_test, ensemble_predictions)\n\n# Convert numeric labels back to class names\nensemble_predictions_names = le.inverse_transform(ensemble_predictions)\nensemble_report = classification_report(y_test_names, ensemble_predictions_names, target_names=class_names)\n\nprint(\"Ensemble Classifier:\")\nprint(f\"Accuracy: {ensemble_accuracy:.2f}\")\nprint(f\"Classification Report:\\n{ensemble_report}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-03-12T11:00:36.776534Z","iopub.execute_input":"2024-03-12T11:00:36.777008Z","iopub.status.idle":"2024-03-12T11:00:38.293297Z","shell.execute_reply.started":"2024-03-12T11:00:36.776977Z","shell.execute_reply":"2024-03-12T11:00:38.291990Z"},"trusted":true},"execution_count":58,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/sklearn/neural_network/_multilayer_perceptron.py:686: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Ensemble Classifier:\nAccuracy: 0.83\nClassification Report:\n                    precision    recall  f1-score   support\n\nPrivate University       0.74      0.74      0.74        39\n Public University       0.88      0.88      0.88        80\n\n          accuracy                           0.83       119\n         macro avg       0.81      0.81      0.81       119\n      weighted avg       0.83      0.83      0.83       119\n\n","output_type":"stream"}]}]}